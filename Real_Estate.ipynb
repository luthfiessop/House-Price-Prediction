{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddb0831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector\n",
    "from sklearn import set_config\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655a0b8f",
   "metadata": {},
   "source": [
    "# Create API connection to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a365cb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create kaggle directory\n",
    "os.makedirs(os.path.expanduser(\"~/.kaggle\"),exist_ok=True)\n",
    "# Set permissions(required by kaggle api)\n",
    "os.chmod(os.path.expanduser(\"~/.kaggle/kaggle.json\"),0o600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e348c1d",
   "metadata": {},
   "source": [
    "## Test the connection to Kaggle before loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa76679",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# Test with a simple call\n",
    "datasets = api.dataset_list(search='titanic')\n",
    "for d in datasets[:3]:\n",
    "    print(d.ref, \"-\", d.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b16b779",
   "metadata": {},
   "source": [
    "# Create a method that downloads and loads the data into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ac7817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_housing_data():\n",
    "    dataset_path=Path(\"/Users/luthfi/sandbox/Kaggle_competitions/Machine_learning/textbook/sandbox/datasets/Real%20estate.csv\")\n",
    "    Path(\"datasets\").mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "    if not dataset_path.exists():\n",
    "        # Authenticate with kaggle api\n",
    "        api = KaggleApi()\n",
    "        api.authenticate()\n",
    "\n",
    "        # Download dataset\n",
    "        api.dataset_download_file(\n",
    "            dataset=\"quantbruce/real-estate-price-prediction\",\n",
    "            file_name=\"Real estate.csv\",\n",
    "            path=\"datasets\",\n",
    "            force=True\n",
    "        )\n",
    "\n",
    "        # Open the file in the local folder created\n",
    "        try:\n",
    "            with open(\"/Users/luthfi/sandbox/Kaggle_competitions/Machine_learning/textbook/sandbox/datasets/Real%20estate.csv\",\"r\") as file:\n",
    "                content= file.read()\n",
    "        except FileNotFoundError:\n",
    "            print(\"File doesn't exist locally\")\n",
    "\n",
    "    return pd.read_csv(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9f5707",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_housing_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abb5146",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81647177",
   "metadata": {},
   "source": [
    "# Check for any missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cff325",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5e7a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8369f289",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(bins=50,figsize=(12,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbb0159",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e625b776",
   "metadata": {},
   "source": [
    "# Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba46767",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set,test_set = train_test_split(df,random_state=42,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3619e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3503d13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4e6625",
   "metadata": {},
   "source": [
    "# Explore + Visualize the data to gain insights - manually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6db9af",
   "metadata": {},
   "source": [
    "## Manual Changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4521fcc",
   "metadata": {},
   "source": [
    "### Change the name so is uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7beca47",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_train = train_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64fd3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2035e0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_train.reset_index(drop=True,inplace=True)\n",
    "manual_train= manual_train.drop(columns='No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8f7da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = list(manual_train.columns)\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162a5557",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(names)):\n",
    "    names[i] = names[i][2:] # This removes the X1\n",
    "    names[i] = names[i].lstrip() # Removes the from space\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06596692",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_train.columns = names\n",
    "manual_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7327f01",
   "metadata": {},
   "source": [
    "### Transaction date - change so its only contains just the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8b5492",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_train['transaction date'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc9c454",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_train['transaction date'] = pd.to_datetime(\n",
    "    manual_train['transaction date'].astype(int), format='%Y'\n",
    ").dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135b43a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57f3ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_train['transaction date']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c273218",
   "metadata": {},
   "source": [
    "### Visualize geographical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd73345",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_train.plot(kind= \"scatter\",x=\"longitude\",y=\"latitude\",grid=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc5791e",
   "metadata": {},
   "source": [
    "Density of points:\n",
    "- When many points overlap, their transparency adds up visually — the more overlap, the darker that region becomes.\n",
    "- This creates a map of where the houses are. With alpha=0.2:\n",
    "\n",
    "    Darker areas = more houses listed there\n",
    "\n",
    "    Lighter/sparse areas = fewer houses\n",
    "- alpha=0.2 helps you:\n",
    "\n",
    "    Spot clusters or hot spots where houses are densely located\n",
    "\n",
    "    Avoid the problem of \"overplotting\" (when points sit on top of each other and hide information)\n",
    "\n",
    "    Make better visual decisions about patterns in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0115a4c",
   "metadata": {},
   "source": [
    "#### Density of points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f21128",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_train.plot(kind=\"scatter\",x=\"longitude\",y=\"latitude\",grid=True,alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2c90cd",
   "metadata": {},
   "source": [
    "### Look for correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4818145",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matric = manual_train.corr()\n",
    "corr_matric['house price of unit area'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c9aef4",
   "metadata": {},
   "source": [
    "### Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2909dba",
   "metadata": {},
   "source": [
    "Transform data to handle skewed data + Reduce the impace of outliers + Make the distribution normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5675080",
   "metadata": {},
   "source": [
    "Plot the distribution to see its skewness, outliers, or non-normality\n",
    "\n",
    "- Histogram : Shows the shape (skew,normal,uniform)\n",
    "- Box plot : To detect outliers (shows median,IQR, and outliers clearly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73706a50",
   "metadata": {},
   "source": [
    "✅ Use StandardScaler when:\n",
    "\n",
    "- You're using models that assume normality or optimize on gradients (e.g., linear models, logistic regression, SVM, PCA)\n",
    "\n",
    "- Your data is approximately normal\n",
    "\n",
    "- You want values centered around 0 and scaled by standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293e5dcb",
   "metadata": {},
   "source": [
    "✅ Use MinMaxScaler when:\n",
    "\n",
    "- You need all features to be in a specific range, like [0, 1] (often for neural networks)\n",
    "\n",
    "- Your data is not normal, but you still need scaling\n",
    "\n",
    "- You're visualizing or applying distance-based models where range matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182dc89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928b4aca",
   "metadata": {},
   "source": [
    "#### transaction date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b392882",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_train['transaction date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c5cf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "\n",
    "sns.histplot(manual_train['transaction date'], kde=True)\n",
    "plt.title('Histogram of transaction date')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b667b76e",
   "metadata": {},
   "source": [
    "Histogram interpretation :\n",
    "- One spike at 2012.0 and another at 2013.0\n",
    "- These nothing in the middle - it looks bimodal and almost like a categorical split between two years\n",
    "- This shows the dates aren't continuous over time - maybe there are transactions only at the start and end of a period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebc0010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box and whisker\n",
    "\n",
    "manual_train.boxplot(column='transaction date')\n",
    "plt.title('Box and whisker of tranasaction date')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7abf959",
   "metadata": {},
   "source": [
    "Box plot interpreatation :\n",
    "- The box is very tall, stretching from 2012 to 2013, and theres very little detail in the quartiles, suggesting only two main groups of dates.\n",
    "- It confirms that theres no smooth distribution, just a jump from one group of dates to another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fef237",
   "metadata": {},
   "source": [
    "Transformer : \n",
    "\n",
    "- No transformers are used for dates in general because its a time based variable.\n",
    "- Dates are not true numerical features — they're temporal, meaning their value is only meaningful in the context of time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e8c469",
   "metadata": {},
   "source": [
    "#### house age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6614d19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(manual_train['house age'],kde=True)\n",
    "plt.title('Histogram of house age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e2430b",
   "metadata": {},
   "source": [
    "Histrogram interpretation :\n",
    "- Distribution is midly skewed to the righ\n",
    "- Most values are concetrated between 0 - 20, but there is a small bump around 30 - 40\n",
    "- No extreme outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fcf5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box and whisker\n",
    "\n",
    "manual_train.boxplot(column='house age')\n",
    "plt.title('Box and whisker for house age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7748c085",
   "metadata": {},
   "source": [
    "Box and whisker :\n",
    "- The data is slightly skewed to the right and the tail is longer on the right.\n",
    "- The medium is not exactly in the middle but more to the left.\n",
    "- No outliers can be seen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed1d6fa",
   "metadata": {},
   "source": [
    "Transformer :\n",
    "No transformer is needed. As the data is more or less uniform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afef61e",
   "metadata": {},
   "source": [
    "#### distance to the nearest MRT station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9447573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "\n",
    "sns.histplot(manual_train[\"distance to the nearest MRT station\"],kde=True)\n",
    "plt.title(\"Histogram for distance to the nearest MRT station\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee56d911",
   "metadata": {},
   "source": [
    "Applying the log trasnformation to see the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cba807",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(8, 3), sharey=True)\n",
    "manual_train[\"distance to the nearest MRT station\"].hist(ax=axs[0],bins=50)\n",
    "manual_train[\"distance to the nearest MRT station\"].apply(np.log).hist(ax=axs[1],bins=50)\n",
    "axs[0].set_xlabel(\"distance to the nearest MRT station\")\n",
    "axs[1].set_xlabel(\"log of the distance to the nearest MRT station\")\n",
    "axs[0].set_ylabel(\"Number of station\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde8e97f",
   "metadata": {},
   "source": [
    "Histogram interpretation :\n",
    "\n",
    "- The histogram clearly shows a right-skewed distribution. A large number of observations are clustered at lower distances (0-1000), and the frequency gradually decreases as the distance increases.\n",
    "\n",
    "- There's a significant tail extending towards higher distances, indicating some properties are much further from an MRT station than the majority.\n",
    "\n",
    "- The presence of the Kernel Density Estimate (KDE) curve further confirms this skewed shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945eed9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box and whisker\n",
    "\n",
    "manual_train.boxplot(column=\"distance to the nearest MRT station\")\n",
    "plt.title(\"Box and whisker for distance to the nearest MRT station\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858ce36b",
   "metadata": {},
   "source": [
    "Box and whisker intrepretation :\n",
    "- The data is highly skewed to the right \n",
    "- There are many outliers\n",
    "- The median is more to the left of the box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f14da25",
   "metadata": {},
   "source": [
    "Transformer : \n",
    "Given the rightly skewed data + outliers log transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a570ee8",
   "metadata": {},
   "source": [
    "#### number of convience stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1df8a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "\n",
    "sns.histplot(manual_train[\"number of convenience stores\"],kde=True)\n",
    "plt.title(\"Histogram for number of convenience stores\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804ef6bf",
   "metadata": {},
   "source": [
    "Histogram interpretation :\n",
    "- There is a high count at 0, a dip, then another peak around 4-6, and then another peak at 6, followed by general decrease.\n",
    "- Counts generally decrease after the peak around 6, though there are still significant counts up to 10.\n",
    "- This is not a clear bell curve. Theres a noticeable frequency of properties with 0 convience stores. The distribution extends up to 10 convience stores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae03ab20",
   "metadata": {},
   "source": [
    "A bimodal distribution is a probability distribution with two distinct peaks (or \"humps\") in its histogram or density plot. You’ll see this as two visible \"hills\" or clusters in the data when plotted.\n",
    "\n",
    "- Unimodal = 1 peak\n",
    "- Bimodal = 2 peaks\n",
    "- Multimodal = 3 or more peaks\n",
    "\n",
    "Think of \"modal\" as “mode” = the most frequent value or region.\n",
    "So multimodal means there are multiple areas where the data tends to cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9989ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box and whisker\n",
    "\n",
    "manual_train.boxplot(column=\"number of convenience stores\")\n",
    "plt.title(\"Box and whisker for number of convenience stores\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82b6586",
   "metadata": {},
   "source": [
    "Box and whisker intrepertation :\n",
    "-  The upper whisker extends all the way to 10, and the lower whisker extends to 0. This indicates that the data is not significantly skewed in the conventional sense, especially not \"rightly skewed.\"\n",
    "- No outliers\n",
    "- the median is 4. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f3f6b1",
   "metadata": {},
   "source": [
    "Transformer : \n",
    "\n",
    "- The histogram shows multiple peaks, suggesting a somewhat uniform or possibly bimodal distribution rather than a strongly skewed one.\n",
    "- Given that the data is not significantly skewed and covers a discrete, limited range (0 to 10), a non-linear transformer (like a log transformer) is generally not needed or recommended.\n",
    "- Log transformations are used to make highly skewed data more symmetrical, which isn't the primary characteristic here. Applying a log transform to data that includes 0 and is not heavily skewed can sometimes be more problematic than beneficial.\n",
    "\n",
    "- No transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89db2cf7",
   "metadata": {},
   "source": [
    "#### Latitude and longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d6b7ea",
   "metadata": {},
   "source": [
    "- If no clustering or distance based tasks then apply only standardization and no custom transformer is needed.\n",
    "\n",
    "- If you are doing clustering KNN or distance-based , don't standardize but rather use a custiner transformer to compute real geographical distances if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856c166a",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3f8898",
   "metadata": {},
   "source": [
    "#### transaction date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e66994",
   "metadata": {},
   "source": [
    "No — you'd use standardization only if the extracted date-based feature is continuous and meaningful for that model (like time intervals). Not because it's a date."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb8cdb7",
   "metadata": {},
   "source": [
    "#### house age"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8134ef",
   "metadata": {},
   "source": [
    "Standardization would be best because the data is roughly normal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a859bea5",
   "metadata": {},
   "source": [
    "#### distance to the nearest MRT station"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1797449",
   "metadata": {},
   "source": [
    "Feature Scaler:\n",
    "\n",
    "- Robust Scaler (sklearn.preprocessing.RobustScaler): After applying a logarithmic transformation, the distribution will be less skewed, but outliers might still have a disproportionate effect on standard scaling methods (like StandardScaler).\n",
    "\n",
    "- RobustScaler scales features using statistics that are robust to outliers. It scales the data according to the Interquartile Range (IQR) and median, which are less affected by extreme values than the mean and standard deviation used by StandardScaler. This is particularly beneficial given the clear outliers observed in your boxplot, even after a potential log transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06db483a",
   "metadata": {},
   "source": [
    "#### number of convience stores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be05e25",
   "metadata": {},
   "source": [
    "- Standard Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee19da1c",
   "metadata": {},
   "source": [
    "#### Latitude and longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584b2921",
   "metadata": {},
   "source": [
    "- If no clustering or distance based tasks then apply only standardization and no custom transformer is needed.\n",
    "\n",
    "- If you are doing clustering KNN or distance-based , don't standardize but rather use a custiner transformer to compute real geographical distances if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f9e5c2",
   "metadata": {},
   "source": [
    "### Scaling the target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d3c6c4",
   "metadata": {},
   "source": [
    "- Its better when all features are on the same scale / range\n",
    "- Works best for linear regression, gradient descent, etc\n",
    "- Helps the model train better - learns is mde easiser for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f51759c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variable = manual_train[\"house price of unit area\"]\n",
    "target_variable.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a133a24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram\n",
    "\n",
    "sns.histplot(target_variable,kde=True)\n",
    "plt.title(\"Histogram for the target variable\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e114eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variable.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8351e7c9",
   "metadata": {},
   "source": [
    "| Skew Value   | Interpretation      | What to Do                               |\n",
    "| ------------ | ------------------- | ---------------------------------------- |\n",
    "| 0.0          | Perfectly symmetric | No transformation needed                 |\n",
    "| 0.0 to ±0.5  | Slight skew         | Scaling usually enough                   |\n",
    "| ±0.5 to ±1.0 | **Moderate skew**   | Consider `StandardScaler`, maybe try log |\n",
    "| > ±1.0       | Strong skew         | Use `log1p()` or another transformation  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067a84cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_manual_train = TransformedTargetRegressor(LinearRegression(),transformer=StandardScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22b19a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_manual_train.fit(manual_train[[\"house price of unit area\"]],target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73df305b",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_new_data = manual_train[[\"house price of unit area\"]].iloc[:5]\n",
    "print(some_new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2915a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_manual_train.predict(some_new_data)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43af40a",
   "metadata": {},
   "source": [
    "## Changes using the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd683bb7",
   "metadata": {},
   "source": [
    "The purpose of fit() and transform():\n",
    "\n",
    "- fit() : learns what must do to the data like the changes -everything\n",
    "- transform() : applys the changes that was done in the fit()\n",
    "- __init__() : only use when you're passing any external parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babedbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_train = train_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04221656",
   "metadata": {},
   "outputs": [],
   "source": [
    "pips = pipe_train.drop(columns=[\"Y house price of unit area\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3876d730",
   "metadata": {},
   "outputs": [],
   "source": [
    "pips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2323e7db",
   "metadata": {},
   "source": [
    "### Change all column names into a uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9787032f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class Column_name_transformer(BaseEstimator,TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.new_columns = [col[2:].lstrip() for col in X.columns]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        x_copy = X.copy()\n",
    "        x_copy.columns = self.new_columns\n",
    "        return x_copy\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return self.new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febc84b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change_name = Column_name_transformer()\n",
    "#pipe_train = change_name.fit_transform(pipe_train)\n",
    "#pipe_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179e93ed",
   "metadata": {},
   "source": [
    "### Transaction date - change to only contain the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cae7a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class transaction_date_transformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column_name):\n",
    "        self.column_name = column_name\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X[self.column_name] = X[self.column_name].astype(int).round()\n",
    "        return X\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        # Return the same column name after transformation\n",
    "        return [self.column_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a121675",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transaction_date_transform = transaction_date_transformer(column_name=\"transaction date\")\n",
    "#pipe_train['transaction date'] = transaction_date_transform.fit_transform(pipe_train)\n",
    "#pipe_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce8d0e3",
   "metadata": {},
   "source": [
    "### drop columns that is not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bc832c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class drop_columns_not_needed(BaseEstimator,TransformerMixin):\n",
    "\n",
    "    def __init__(self,number_columns):\n",
    "        self.number_columns=number_columns\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        X.reset_index(drop=True,inplace=True)\n",
    "        self.new_x = X.iloc[:,self.number_columns:]\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X):\n",
    "        return self.new_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3871ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping_index_columns = drop_columns_not_needed(number_columns=1)\n",
    "#pipe_train = dropping_index_columns.fit_transform(pipe_train)\n",
    "#pipe_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096261d7",
   "metadata": {},
   "source": [
    "## Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9161973",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pipeline=make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"),\n",
    "    FunctionTransformer(np.log,feature_names_out=\"one-to-one\"),\n",
    "    RobustScaler()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceb1fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"),\n",
    "                                     StandardScaler())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c5ae43",
   "metadata": {},
   "source": [
    "Call all the custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644d54c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_name = Column_name_transformer()\n",
    "transaction_date_transform = transaction_date_transformer(column_name=\"X1 transaction date\")\n",
    "dropping_index_columns = drop_columns_not_needed(number_columns=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23018f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_pipe = ColumnTransformer([\n",
    "#    (\"log\", log_pipeline, [\"distance to the nearest MRT station\", \"house price of unit area\"])\n",
    "#], remainder='passthrough')\n",
    "\n",
    "#test_pipe.fit_transform(pipe_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53a79d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transaction_date = transaction_date_transformer(column_name=\"transaction date\")\n",
    "\n",
    "#test_pipe = ColumnTransformer([\n",
    "#    (\"transaction_date_transform\", transaction_date, [\"transaction date\"])\n",
    "#], remainder=\"passthrough\")\n",
    "\n",
    "#test_pipe.fit_transform(pipe_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743f0224",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropper = drop_columns_not_needed(number_columns=1)\n",
    "\n",
    "#test_pipe = ColumnTransformer([\n",
    "#    (\"drop_index\", dropper, pipe_train.columns.tolist())  # or just [\"column1\", \"column2\", ...]\n",
    "#], remainder=\"passthrough\")\n",
    "\n",
    "#test_pipe.fit_transform(pipe_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012fbb62",
   "metadata": {},
   "source": [
    "## Apply the preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d52cb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Drop the 'No' column\n",
    "X = pipe_train.drop(columns=[\"No\", \"Y house price of unit area\"])\n",
    "y = pipe_train[\"Y house price of unit area\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00edd6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Use updated pipeline\n",
    "final_pipeline = make_pipeline(\n",
    "    Column_name_transformer(),\n",
    "    transaction_date_transformer(\"transaction date\"),\n",
    "    ColumnTransformer([\n",
    "        (\"log\", log_pipeline, [\"distance to the nearest MRT station\"]),\n",
    "    ], remainder=default_num_pipeline),\n",
    "    LinearRegression()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3068ce",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b2e548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train model\n",
    "final_pipeline.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ee8a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Predict\n",
    "housing_predictions = final_pipeline.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7935dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Check first 5 predictions\n",
    "print(\"Predictions:\", housing_predictions[:5].round(1))\n",
    "print(\"True values:\", y.iloc[:5].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c08106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Relative Error\n",
    "error_ratios = housing_predictions[:5] / y.iloc[:5].values - 1\n",
    "print(\"Errors:\", \", \".join([f\"{100 * ratio:.1f}%\" for ratio in error_ratios]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2879dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: RMSE\n",
    "lin_rmse = np.sqrt(mean_squared_error(y, housing_predictions))\n",
    "print(\"Linear Regression RMSE:\", round(lin_rmse, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a24f8e1",
   "metadata": {},
   "source": [
    "## Data after it went through the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce78336c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the preprocessing pipeline (exclude model step)\n",
    "preprocessor_only = make_pipeline(\n",
    "    Column_name_transformer(),\n",
    "    transaction_date_transformer(\"transaction date\"),\n",
    "    ColumnTransformer([\n",
    "        (\"log\", log_pipeline, [\"distance to the nearest MRT station\"]),\n",
    "    ], remainder=default_num_pipeline)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540cd75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on X\n",
    "preprocessor_only.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f4ee7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform X\n",
    "X_prepared = preprocessor_only.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145b357c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names\n",
    "column_transformer = preprocessor_only.named_steps[\"columntransformer\"]\n",
    "feature_names = column_transformer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e359f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put into DataFrame\n",
    "X_prepared_df = pd.DataFrame(X_prepared, columns=feature_names)\n",
    "\n",
    "print(\"After preprocessing:\")\n",
    "print(X_prepared_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31536f74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce2c07e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3bb92e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6255242e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d7213d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b350765",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9041d42d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
